
#  Assignment Email Ham or Spam
##  Reproducible notes for Email spam or not

#            Anil Kumar  IIT Madras



## [[source files available on GitHub](https://github.com/anilcs13m)]]
## [[connect on linkedin]](https://in.linkedin.com/in/anilcs13m)]]

## PRELIMINARIES

Load the library that are required in the assignment:
```{r load_packages, cache = FALSE, echo = TRUE, message = FALSE, warning = FALSE, tidy = FALSE}
library("tm")
library("SnowballC")

library("caTools")
library("rpart")
library("rpart.plot")
library("ROCR")
library("randomForest")

```


## INTRODUCTION

Nearly every email user has at some point encountered a "spam" email, which is an unsolicited message often advertising a
product, containing links to malware, or attempting to scam the recipient. Roughly 80-90% of more than 100 billion emails
sent each day are spam emails, most being sent from botnets of malware-infected computers. The remainder of emails are
called "ham" emails.

As a result of the huge number of spam emails being sent across the Internet each day, most email providers offer a spam
filter that automatically flags likely spam messages and separates them from the ham. Though these filters use a number
of techniques, most rely heavily on the analysis of the contents of an email via text analytics.

We will be looking into how to use the text of emails is spam or ham.

We will be extracting word frequencies from the text of the documents, and then integrating those
frequencies into predictive models.  

We are going to talk about __predictive coding__ -- an emerging use of text analytics in the area
of criminal justice.


### The _eDiscovery_ Problem 

we will build and evaluate a spam filter using a publicly available dataset first described in the 2006 conference paper 
"Spam Filtering with Naive Bayes -- Which Naive Bayes?" by V. Metsis, I. Androutsopoulos, and G. Paliouras. The "ham" messages 
in this dataset come from the inbox of former Enron Managing Director for Research Vincent Kaminski, one of the inboxes 
in the Enron Corpus. One source of spam messages in this dataset is the SpamAssassin corpus, which contains hand-labeled 
spam messages contributed by Internet users. The remaining spam was collected by Project Honey Pot, a project that collects spam messages and identifies spammers by publishing email address that humans would know not to contact but that bots might target with spam.

### Predictive Coding

__Predictive coding__ is a new technique in which attorneys manually label some documents and then
use text analytics models trained on the manually labeled documents to predict which of the
remaining documents are responsive.


### The Data

The data set contains just two fields:

* __text__: the text of the email in question, 
* __spam__: a binary (0/1) variable telling whether the email was spam.

## LOADING THE DATA

```{r loading_data}
emails = read.csv("emails.csv", stringsAsFactors=FALSE)
```
## Let's do the some analytics of the data
```{r check_df}
str(emails)
```
Let's look at a few examples (using the `strwrap()` function for easier-to-read formatting):
```{r example_email_1}
strwrap(emails$text[1])
```

Let's check the second email: 
```{r example_email_2}
strwrap(emails$text[2])
```
We can check this in the value of the `responsive` variable.
```{r example_email_2_responsive}
emails$spam[2]
```

Which word appears at the beginning of every email in the dataset
```{r subject}
emails$text[1]
```
How many characters are in the longest email in the dataset
where longest is measured in terms of the maximum number of characters?
```{r longest}
max(nchar(emails$text))
```
print the longest email present in the dataset 
```{r print_longest}
min(nchar(emails$text))
which(nchar(emails$text) == 13)
```
Which row contains the shortest email in the dataset?
```{r shortest}
which.min(nchar(emails$text))
```

Let's look at the breakdown of the number of emails are spam and not.
```{r table_of_responsive}
table(emails$spam)
```
We see that the data set is unbalanced, with a relatively small proportion of emails responsive to
the query.  This is typical in predictive coding problems.


## CREATING A CORPUS

Follow the standard steps to build and pre-process the corpus:

1) Build a new corpus variable called corpus.

2) Using tm_map, convert the text to lowercase.

3) Using tm_map, remove all punctuation from the corpus.

4) Using tm_map, remove all English stopwords from the corpus.

5) Using tm_map, stem the words in the corpus.

6) Build a document term matrix from the corpus, called dtm.

we are calling our corpus as the dtm

# first step 
```{r create_corpus}

corpus <- Corpus(VectorSource(emails$text))
```

Let's take a look at corpus:
```{r check_corpus}
corpus
```
# preprocessing the corpus
We use the `tm_map()` function which takes as

* its first argument the name of a __corpus__ and 
* as second argument a __function performing the transformation__ that we want to apply to the text.

```{r preprocessing}
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
```

### Removing _stop words_ 

Removing words can be done with the `removeWords` argument to the `tm_map()` function, with an
extra argument, _i.e._ what the stop words are that we want to remove, for which we simply 
use the list for english that is provided by the `tm` package.

We will remove all of these English stop words, but we will also remove the word "_apple_"
since all of these tweets have the word "_apple_" and it probably won't be very useful in our
prediction problem.

```{r process_remove_stopwords}
corpus <- tm_map(corpus, removeWords, stopwords("english"))
```

### Stemming

Lastly, we want to stem our document with the `stemDocument` argument.
```{r process_stemming}
corpus <- tm_map(corpus, stemDocument)
```
It is a lot harder to read now that we removed all the stop words and punctuation and word stems,
but now the emails in this corpus are ready for our machine learning algorithms.

## BAG OF WORDS

### Create a _Document Term Matrix_

We are now ready to extract the __word frequencies__ to be used in our prediction problem.
The `tm` package provides a function called `DocumentTermMatrix()` that generates a __matrix__ where:

* the __rows__ correspond to __documents__, and 
* the __columns__ correspond to __words__ .

The values in the matrix are the number of times that word appears in each document.

```{r dtm}
dtm = DocumentTermMatrix(corpus)
```
#  view the document term matrix
```{r view}
dtm
```
To obtain a more reasonable number of terms, limit dtm to contain terms appearing in at least 5% of documents, and store
this result as spdtm, and view the number of terms present in the spdtm?

So we want to remove the terms that don't appear too often in our data set.

### Remove sparse terms

```{r sparse}
spdtm = removeSparseTerms(dtm, 0.95)
```
Now we can take a look at the summary statistics for the document-term matrix:
```{r check_lighter_DTM}
spdtm
```
We can see that we have decreased the number of terms to __`r spdtm$ncol`__, 
which is a much more reasonable number.

### Creating a data Frame from the _DTM_
Build a data frame called __emailsSparse__ from _spdtm_, and use the __make.names__ function to make the variable names of
emailsSparse valid.

```{r data_frame}
emailsSparse = as.data.frame(as.matrix(spdtm))
```
### use the _make.names_ function to make the variable names of emailsSparse valid.
```{r make_names}
colnames(emailsSparse) = make.names(colnames(emailsSparse))
```
# sort the words

__colSums()__ is an R function that returns the sum of values for each variable in our data frame. Our data frame contains the
number of times each word stem (columns) appeared in each email (rows). Therefore, __colSums(emailsSparse)__ returns the
number of times a word stem appeared across all the emails in the dataset. What is the word stem that shows up most
frequently across all the emails in the dataset? Hint: think about how you can use __sort()__ or __which.max()__ to pick out the
maximum frequency.

What is the word stem that shows up most frequently across all the emails in the dataset
```{r frequncy}
sort(colSums(emailsSparse))
which.max(colSums(emailsSparse))
```
# Adding the variable
Add a variable called "spam" to emailsSparse containing the email spam labels, this can be done by copying over the "spam"
variable from the original data frame.
```{r copy}
emailsSparse$spam = emails$spam
```
Now let's see how many time word stems appear at least 5000 times in the ham emails in the dataset
We can read the most frequent terms in the ham dataset
```{r freq}
sort(colSums(subset(emailsSparse, spam == 0)))
```
# dataset to the spam emails
```{r spam_view}
# subset(emailsSparse, spam == 1)
```
# we can read the most frequent terms 
```{r read1}
sort(colSums(subset(emailsSparse, spam == 1)))
```
## Build Machine learnig model

First, convert the dependent variable to a factor with __emailsSparse$spam = as.factor(emailsSparse$spam)__
```{r factor_spam}
emailsSparse$spam = as.factor(emailsSparse$spam)
```
we are setting the random seed some value so that every time same result will come.
```{r seed_set}
set.seed(123)
```
# Building the model
before building the model we need to split our data into training and testing by using __sample.split__ function with 70 data in training and rest in test
```{r split_data}
spl = sample.split(emailsSparse$spam, 0.7)
```
# Train and Test Subset 
use the __subset__ function __TRUE__ for the train and __FALSE__ for the test
```{r train_test}
train = subset(emailsSparse, spl == TRUE)
test = subset(emailsSparse, spl == FALSE)
```
# Logistic Regression Model

we are creating a logistic regression model called as spamLog, for logistic regression we use function __glm__ with family binomial.
we are using the all the variable as independent variable for training our model, for detection.

```{r lgm_model}
spamLog = glm(spam~., data=train, family="binomial")
```
# summary
summary of logistic model spamLog
```{r summary_Log1}
# summary(spamLog)
```
# BUILD A CART MODEL

create one model __CART__ and called this model as __spamCART__, we are using default parameters to train this model, so that no need to add __minbucket__ or __cp__ parameters. Remember to add the argument method="class" since this is a binary classification problem.
```{r cart_model}

spamCART = rpart(spam~., data=train, method="class")
```
# PLOT CART

```{r CART_plot_tree, fig.width = 5, fig.height = 4}
prp(spamCART)
```
# BUILD A RANDOM FOREST
We are creating one more model random forest model and called this model as __spamRF__, we are using default parameters to train this mode also, no need to worry about specifying ntree or nodsize. Directly before training the random forest model, set the random seed to 123 (even though we've already done this, it's important to set the seed right before training the model so we all obtain the
same results. Keep in mind though that on certain operating systems, your results might still be slightly different).
```{r seed_again}
set.seed(123)
```
### Create Random Forest Model using all the parameters.
```{r rf_model}
spamRF = randomForest(spam~., data=train)
```

### Out-of-Sample Performance of all the above model we created

Now that we have trained a models, we need to evaluate it on the test set.  
For each model, obtain the predicted spam probabilities for the training set. Be careful to obtain probabilities instead of
predicted classes, because we will be using these values to compute training set __AUC__ values. Recall that you can obtain
probabilities for CART models by not passing any type parameter to the predict() function, and you can obtain probabilities
from a random forest by adding the argument __type="prob"__. For CART and random forest, you need to select the second
column of the output of the __predict()__ function, corresponding to the probability of a message being spam

* The __first__ column is the predicted probability of the document being __non-responsive__.
* The __second__ column is the predicted probability of the document being __responsive__.
* They sum to 1.
In our case we are interested in the predicted probability of the document being responsive, so for that we are using __second__.

```{r predict_models}
predTrainLog = predict(spamLog, type="response")
predTrainCART = predict(spamCART)[,2]
predTrainRF = predict(spamRF, type="prob")[,2]
```
This new object gives us the predicted probabilities on the test set.
# predicted probabilities

How many of the training set predicted probabilities from spamLog are less than 0.00001?
```{r spamLog1}
table(predTrainLog<0.00001)
```
How many of the training set predicted probabilities from spamLog are more than 0.99999?
```{r spamLog}
table(predTrainLog > 0.99999)
```
How many of the training set predicted probabilities from spamLog are between 0.00001 and 0.99999?
```{r spamlog2}
table(predTrainLog >= 0.00001 & predTrainLog <= 0.99999)
```
# summary
```{r summary_Log}
# summary(spamLog)
```
# Accuracy

What is the training set accuracy of spamLog, using a threshold of 0.5 for predictions.
```{r accuracy_spamLog}
table(train$spam, predTrainLog > 0.5)
(3052+954)/nrow(train)
```
What is the training set AUC of spamLog?
```{r auc_spamLog}
predictionTrainLog = prediction(predTrainLog, train$spam)
as.numeric(performance(predictionTrainLog, "auc")@y.values)
```
What is the training set accuracy of spamCART, using a threshold of 0.5 for predictions?
```{r accuracy_spamCART}
table(train$spam, predTrainCART > 0.5)
```
# accuracy
```{r accuracy_CART}
(2885+894)/nrow(train)
```
What is the training set AUC of spamCART?
```{r accuracy_1}
predictionTrainCART = prediction(predTrainCART, train$spam)
as.numeric(performance(predictionTrainCART, "auc")@y.values)
```
What is the training set accuracy of spamRF, using a threshold of 0.5 for predictions?
```{r accuracy_RF}
table(train$spam, predTrainRF > 0.5)
```
# accuracy 
```{r calculation}
(3013+914)/nrow(train)
```

# EVALUATING ON THE TEST SET
We are interested in the __accuracy__ of our models on the test set, _i.e._ out-of-sample.
First we compute the _confusion matrix_:
```{r test_predict}
predTestLog = predict(spamLog, newdata=test, type="response")
predTestCART = predict(spamCART, newdata=test)[,2]
predTestRF = predict(spamRF, newdata=test, type="prob")[,2]
```
# Test Accuracy of all the above model

testing set accuracy of spamLog, using a threshold of 0.5 for predictions?
```{r test_accuracy}
table(test$spam, predTestLog > 0.5)
```
# accuracy
```{r accuracy_test}
(1257+376)/nrow(test)
```
What is the testing set AUC of spamLog
```{r test_accuracy_auc}
predictionTestLog = prediction(predTestLog, test$spam)
as.numeric(performance(predictionTestLog, "auc")@y.values)
```
What is the testing set accuracy of spamCART, using a threshold of 0.5 for predictions?
```{r test_accuracy_CART}
table(test$spam, predTestCART > 0.5)
```
# accuracy 
```{r accuracy_CATR1}
(1228+386)/nrow(test)
```
# What is the testing set AUC of spamCART?
```{r accuracy_AUC1}
predictionTestCART = prediction(predTestCART, test$spam)
as.numeric(performance(predictionTestCART, "auc")@y.values)
```

What is the testing set accuracy of spamRF, using a threshold of 0.5 for predictions?
```{r RF1}
table(test$spam, predTestRF > 0.5)
```
# accuracy 
```{r accuracy_RF1}
(1290+385)/nrow(test)
```
# What is the testing set AUC of spamRF?
```{r test_RF}
predictionTestRF = prediction(predTestRF, test$spam)
as.numeric(performance(predictionTestRF, "auc")@y.values)
```
